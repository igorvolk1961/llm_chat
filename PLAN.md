# План реализации модульного приложения FastAPI для чата с LLM

## Общая архитектура

Приложение будет состоять из следующих компонентов:
1. Backend на FastAPI с OpenAI-совместимым API
2. Frontend (веб-интерфейс) с двухпанельным UI
3. Система конфигурации (YAML)
4. Система управления контекстами и статистикой
5. Система управления промптами

---

## Этап 1: Структура проекта и конфигурация

### 1.1. Создание структуры директорий
- `app/` - основной код приложения
  - `api/` - API endpoints
  - `models/` - модели данных
  - `services/` - бизнес-логика
  - `config/` - управление конфигурацией
  - `storage/` - работа с файлами (контексты, промпты, статистика)
- `frontend/` - веб-интерфейс
- `config/` - конфигурационные файлы
- `prompts/` - папка с промптами
- `contexts/` - папка с контекстами диалогов
- `requirements.txt` - зависимости Python

### 1.2. Создание конфигурационных файлов
- `config.yaml` - основной конфиг приложения
  - `model_config_path` - путь к конфигу модели
  - `contexts_dir` - путь к папке контекстов
  - `stats_dir` - путь к папке статистики (может совпадать с contexts_dir)
  - `prompts_dir` - путь к папке с промптами
  - `system_prompt_path` - путь к файлу системного промпта (используется в приоритете над model_config)
  - `logging` - конфигурация логирования (опционально)
    - `level` - уровень логирования (DEBUG, INFO, WARNING, ERROR, CRITICAL), по умолчанию INFO
    - `format` - формат сообщений лога
    - `file` - путь к файлу логов (опционально)
    - `console` - вывод логов в консоль (по умолчанию true)
    - `append` - добавлять логи в конец файла (true) или перезаписывать файл при запуске (false, по умолчанию)
- `model_config.yaml` - конфигурация модели
  - `provider_url` - URL провайдера (включая "/v1")
  - `api_key` - API ключ для аутентификации
  - `model_name` - название модели
  - `temperature` - температура
  - `max_tokens` - максимальная длина ответа
  - `system_prompt_path` - путь к файлу системного промпта по умолчанию (используется если не указан в основном конфиге)

### 1.3. Создание requirements.txt
- fastapi
- uvicorn
- pydantic
- pyyaml
- httpx (для запросов к LLM провайдеру)
- python-multipart (для загрузки файлов)

---

## Этап 2: Backend - базовая инфраструктура

### 2.1. Модели данных (Pydantic)
- `Message` - сообщение в контексте согласно OpenAI API:
  - `role` (обязательное): "system" | "user" | "assistant" | "tool"
  - `content` (опциональное): string | null (может быть null для assistant messages только с tool_calls, но может быть не пустым одновременно с tool_calls)
  - `name` (опциональное): string (для function/tool messages)
  - `tool_calls` (опциональное): массив ToolCall (для assistant messages, может быть одновременно с не пустым content)
  - `tool_call_id` (опциональное): string (для tool messages)
- `ToolCall` - вызов инструмента (id, type, function)
- `ChatRequest` - запрос к API чата
- `ChatResponse` - ответ от API чата
- `Metadata` - метаданные ответа (timing, tokens, etc.)
- `Config` - конфигурация приложения
- `ModelConfig` - конфигурация модели

### 2.2. Система конфигурации
- Модуль загрузки и валидации `config.yaml`
- Модуль загрузки и валидации `model_config.yaml`
- Класс для управления конфигурацией
- Модель `LoggingConfig` для конфигурации логирования
- Модуль настройки логирования (`app/config/logging_config.py`)
- Инициализация логирования при старте приложения

### 2.3. Сервис работы с файлами
- Загрузка/сохранение промптов
- Загрузка/сохранение контекстов (JSON)
- Загрузка/сохранение статистики (JSON)
- Автоматическое сохранение текущих данных
- **Автоматическое обновление пути к системному промпту:**
  - При сохранении промпта с именем, совпадающим с текущим системным промптом
  - Путь автоматически обновляется в основном конфигурационном файле (`config.yaml`)
  - Используется регулярное выражение для сохранения форматирования и комментариев

---

## Этап 3: Backend - API endpoints

### 3.1. OpenAI-совместимые endpoints
- `POST /v1/chat/completions` - основной endpoint для чата
- Поддержка streaming (опционально)

### 3.2. Endpoints для управления промптами
- `GET /api/prompts` - список доступных промптов
- `GET /api/prompts/{name}` - загрузка промпта
- `POST /api/prompts/{name}` - сохранение промпта
- `DELETE /api/prompts/{name}` - удаление промпта

### 3.3. Endpoints для управления контекстами
- `GET /api/contexts` - список контекстов
- `GET /api/contexts/{name}` - загрузка контекста
- `POST /api/contexts/{name}` - сохранение контекста
- `DELETE /api/contexts/{name}` - удаление контекста
- `POST /api/contexts/{name}/rename` - переименование контекста

### 3.4. Endpoints для текущего состояния
- `GET /api/current/prompt` - текущий промпт
- `GET /api/current/system-prompt` - текущий системный промпт (загружается из основного конфига, если указан, иначе из конфига модели)
- `GET /api/current/tools` - текущие tools
- `GET /api/current/context` - текущий контекст
- `GET /api/current/content` - текущий ответ (content)
- `GET /api/current/tool-call` - текущий ответ (tool_call)
- `GET /api/current/stats` - текущая статистика
- `POST /api/current/*` - обновление соответствующих данных

### 3.5. Endpoints для управления конфигурацией
- `GET /api/config` - получить текущую конфигурацию (структурированные данные)
- `GET /api/config/raw` - получить сырое содержимое конфигурационного файла (YAML)
- `POST /api/config/raw` - сохранить сырое содержимое конфигурационного файла (YAML)
- Валидация YAML перед сохранением
- Автоматическая перезагрузка конфигурации после сохранения

---

## Этап 4: Backend - интеграция с LLM

### 4.1. Flow работы с инструментами (tools)

**Важно:** Инструменты (tools) являются опциональными. Приложение должно работать в двух режимах:
- **Без инструментов** - обычный диалог, модель возвращает только content
- **С инструментами** - модель может вызывать функции, требуется обработка tool_calls

Согласно OpenAI API, работа с инструментами происходит в несколько этапов:

1. **Первый запрос** - пользователь отправляет запрос (с опциональным определением tools):
   - В запросе может быть указан массив `tools` с определениями функций (опционально)
   - Если tools не указаны - модель работает в обычном режиме, возвращая только content
   - Если tools указаны - модель анализирует запрос и может решить вызвать инструмент (но не обязана)

2. **Ответ модели с tool_calls**:
   - Модель возвращает message с `role="assistant"`
   - В message присутствует массив `tool_calls` с вызовами инструментов
   - Каждый tool_call содержит:
     - `id` - уникальный идентификатор вызова
     - `type` - тип (обычно "function")
     - `function` - объект с `name` и `arguments` (JSON строка)
   - `content` может быть null или содержать текст одновременно с tool_calls

3. **Обработка tool_calls на стороне приложения**:
   - Приложение получает tool_calls из ответа
   - Выполняет соответствующие функции с переданными аргументами
   - Формирует результаты выполнения

4. **Отправка результатов обратно модели**:
   - Создается новый message с `role="tool"`
   - Указывается `tool_call_id`, соответствующий `id` из tool_calls
   - Указывается `name` функции
   - В `content` помещается результат выполнения (обычно JSON строка)
   - Этот message добавляется в контекст диалога

5. **Повторный запрос к модели**:
   - Отправляется весь контекст диалога, включая tool message
   - Модель получает результаты инструментов и формирует финальный ответ
   - Модель может вызвать другие инструменты или завершить диалог

**Важные моменты:**
- **Инструменты опциональны** - приложение должно работать и без tools
- Если tools не переданы в запросе - модель работает в обычном режиме
- Если tools переданы, но модель не вызвала их - ответ содержит только content
- Модель может вызвать несколько инструментов одновременно
- После получения результатов инструментов модель может продолжить генерацию текста
- Модель может вызвать инструменты несколько раз в одном диалоге
- Каждый tool_call имеет уникальный id, который используется в tool message

### 4.2. Сервис LLM клиента
- Класс для работы с провайдером LLM
- Использование api_key из конфигурации для аутентификации в заголовках запросов (Authorization: Bearer {api_key})
- Метод отправки запроса к LLM провайдеру
- Обработка ответов:
  - **С content** - обычный ответ модели (может быть с tools или без)
  - **С tool_calls** - ответ с вызовами инструментов (опционально)
  - **Одновременно content и tool_calls** - модель может вернуть и текст, и вызовы инструментов
- Извлечение метаданных (timing, tokens)
- Поддержка HTTP запросов через httpx
- Поддержка работы без tools (tools=None в запросе)

### 4.3. Обработка статистики
- Вычисление скорости инференса (tokens/sec)
- Временные параметры (latency, time_to_first_token, total_time)
- Длина ответа (tokens, words, characters)
- Средняя длина токена в символах (response_characters / response_tokens)
- Средняя длина слова в токенах (response_tokens / response_words)
- Длина контекста в токенах (из usage.prompt_tokens)
- Сохранение статистики в current_stats.json

### 4.4. Обработка tool calls
- Проверка наличия tool_calls в ответе (опциональное поле)
- Если tool_calls отсутствуют - сохраняется только content
- Парсинг tool_calls из ответа модели (если присутствуют)
- Сохранение tool_calls в формате JSON в current_tool_call.json (если есть)
- Поддержка множественных tool_calls в одном ответе
- Извлечение информации о вызванных функциях и аргументах
- Обработка случая, когда модель не вызвала инструменты, даже если они были переданы

---

## Этап 5: Frontend - базовая структура

### 5.1. Выбор технологии
- Вариант 1: HTML + JavaScript (vanilla или с легким фреймворком)
- Вариант 2: React/Vue (если нужна более сложная логика)
- Рекомендация: начать с простого HTML + JavaScript для быстрого прототипа

### 5.2. Базовая разметка
- Две панели, разделенные вертикальным сплитером
- Верхняя панель: табы (Промпт, Системный промпт, tools, Контекст, Конфигурация)
- Нижняя панель: табы (Ответ (content), Ответ (tool_call), Метаданные)

### 5.3. Стилизация
- CSS для современного UI
- Адаптивный дизайн
- Стили для сплитера, табов, гармошки контекста

---

## Этап 6: Frontend - функциональность верхней панели

### 6.1. Вкладка "Промпт"
- **Отображение пути к файлу:** над текстовым полем отображается путь к текущему промпту (`prompts_dir/{имя_промпта}.txt`), по умолчанию - "Новый промпт"
- Текстовое поле для ввода промпта
- Кнопки: Сохранить, Загрузить, Очистить
- **Кнопка "Загрузить":** открывает диалог выбора файла с диска (по умолчанию предлагается папка `prompts`)
- При выборе файла его содержимое загружается в текстовое поле, путь обновляется на имя выбранного файла
- При сохранении промпта путь обновляется на сохраненное имя файла
- При очистке промпта путь сбрасывается на "Новый промпт"

### 6.2. Вкладка "Системный промпт"
- **Отображение пути к файлу:** над текстовым полем отображается путь к текущему системному промпту из конфигурации (`system_prompt_path`)
- Текстовое поле для системного промпта
- Кнопки: Сохранить, Загрузить, Загрузить системный промпт по умолчанию, Очистить
- **Автоматическая загрузка при старте:** при инициализации приложения автоматически загружается системный промпт из пути, указанного в основном конфиге (`config.yaml`), если файл не найден - из конфига модели (`model_config.yaml`)
- **Кнопка "Загрузить":** открывает диалог выбора файла с диска (по умолчанию предлагается папка `prompts`)
  - При выборе файла его содержимое загружается в текстовое поле
  - **Автоматическое обновление конфигурации:** при выборе файла путь к системному промпту в основной конфигурации (`config.yaml`) автоматически обновляется:
    - Если файл находится в папке `prompts` - путь обновляется на `prompts/{имя_файла}`
    - Если файл находится вне папки `prompts` - файл копируется в папку `prompts`, затем путь обновляется
- **Кнопка "Загрузить системный промпт по умолчанию":** загружает системный промпт из пути, указанного в основном конфиге (`config.yaml`), если файл не найден - из конфига модели (`model_config.yaml`)
- **Автоматическое обновление пути:** при сохранении промпта с именем, совпадающим с текущим системным промптом, путь в основном конфиге обновляется автоматически

### 6.3. Вкладка "tools"
- Редактор JSON для tools (опционально)
- Валидация JSON перед отправкой
- Кнопки: Сохранить, Загрузить, Очистить
- **Кнопка "Загрузить":** открывает диалог выбора файла с диска (по умолчанию предлагается папка `prompts`)
- **Важно:** Tools опциональны - можно оставить поле пустым для работы без инструментов
- Если поле пустое или содержит невалидный JSON - запрос отправляется без tools
- Если tools указаны, но модель не вызвала их - это нормальное поведение

### 6.4. Вкладка "Контекст"
- **Отображение пути к файлу:** над гармошкой отображается полный путь к текущему файлу контекста (`contexts_dir/{имя_контекста}.json`)
- Гармошка (accordion) для messages
- Каждый message содержит:
  - role (select): system, user, assistant, tool
  - content (textarea с markdown preview, может быть пустым для assistant messages только с tool_calls, но может быть не пустым одновременно с tool_calls)
  - name (опциональное поле для function/tool messages)
  - tool_calls (опциональное, для assistant messages) - редактор JSON (может быть одновременно с не пустым content)
  - tool_call_id (опциональное, для tool messages)
- Кнопки: Добавить message, Удалить message, Сохранить контекст
- Автоматическая синхронизация с файлом контекста
- Поле для имени файла контекста (по умолчанию - временная метка)
- Выпадающий список доступных контекстов
- Валидация структуры message в соответствии с OpenAI API
- **Работа с tool calls:**
  - После получения ответа с tool_calls, пользователь может вручную добавить tool messages в контекст
  - При добавлении message с role="tool" необходимо указать tool_call_id, соответствующий id из tool_calls
  - Указать name функции и content с результатом выполнения
  - После добавления tool messages можно отправить запрос снова для получения финального ответа модели

### 6.5. Вкладка "Конфигурация"
- **Отображение пути к файлу:** над редактором отображается путь к файлу конфигурации (`config/config.yaml`)
- Редактор YAML для основного конфигурационного файла (`config.yaml`)
- Отображение текущей конфигурации приложения
- Возможность редактирования всех полей конфигурации
- Валидация YAML перед сохранением
- Кнопка: Сохранить конфигурацию
- **Автоматическое обновление:** содержимое вкладки автоматически обновляется при изменении системного промпта (через кнопку "Загрузить" во вкладке "Системный промпт")
- После сохранения конфигурации приложение перезагружается для применения изменений

---

## Этап 7: Frontend - функциональность нижней панели

### 7.1. Вкладка "Ответ модели (content)"
- Отображение content ответа
- Рендеринг markdown
- Автоматическое сохранение в current_content.md

### 7.2. Вкладка "Ответ модели (tool_call)"
- Отображение tool_calls из ответа модели (если присутствуют)
- Если tool_calls отсутствуют - отображается сообщение "Инструменты не использованы"
- **Не загружается при инициализации:** содержимое загружается только после получения ответа от модели
- **Гармошка (accordion) для каждого tool_call:**
  - Заголовок раздела: `[tool_call_id] function_name` (например, `[call_abc123] get_weather`)
  - Содержимое каждого раздела:
    - **Информация о tool_call** (только чтение):
      - ID: `call_abc123` (tool_call_id)
      - Функция: `get_weather` (name)
      - Аргументы: красиво отформатированный JSON с подсветкой синтаксиса
    - **Форма для создания tool message:**
      - `tool_call_id` - предзаполнен из tool_call.id (только чтение)
      - `name` - предзаполнен из tool_call.function.name (только чтение)
      - `content` - текстовое поле для ввода результата выполнения функции (обычно JSON строка)
    - Кнопка "Добавить tool message в контекст" - добавляет заполненный tool message в контекст диалога
- Пользователь может заполнить ответы на все tool_calls или только на некоторые
- После добавления tool message в контекст, он появляется во вкладке "Контекст"
- Автоматическое сохранение tool_calls в current_tool_call.json

### 7.3. Вкладка "Метаданные и статистика"
- Отображение метаданных ответа
- Статистические данные:
  - Скорость инференса (tokens/sec)
  - Временные параметры (latency, time_to_first_token, total_time)
  - Длина ответа (tokens, words, characters)
  - Средняя длина токена в символах
  - Средняя длина слова в токенах
  - Длина контекста в токенах
- Автоматическое сохранение в current_stats.json
- **Не загружается при инициализации:** содержимое загружается только после получения ответа от модели

---

## Этап 8: Frontend - интеграция с Backend

### 8.1. API клиент
- Функции для всех API endpoints
- Обработка ошибок
- Индикаторы загрузки

### 8.2. Отправка запроса к LLM
- Кнопка "Отправить" для отправки запроса
- Сбор данных из всех вкладок верхней панели:
  - Промпт из вкладки "Промпт"
  - Системный промпт из вкладки "Системный промпт"
  - Tools из вкладки "tools" (опционально, если указаны и валидны)
  - Контекст (messages) из вкладки "Контекст"
- Формирование запроса:
  - Если есть промпт, добавляется user message с промптом
  - **Автоматическое добавление системного промпта:** при первом запросе, если системный промпт указан и отсутствует в контексте, он автоматически добавляется в контекст как system message и вкладка "Контекст" обновляется
  - Все messages из контекста добавляются в запрос
  - **Tools добавляются в запрос только если указаны и валидны** (опциональное поле)
  - Если tools не указаны или пусты - запрос отправляется без tools (обычный режим)
- Отправка запроса к `/v1/chat/completions`
- Обработка ответа:
  - Если ответ содержит content - отображается во вкладке "Ответ (content)"
  - Если ответ содержит tool_calls - отображается во вкладке "Ответ (tool_call)"
  - Если tool_calls отсутствуют - вкладка "Ответ (tool_call)" показывает соответствующее сообщение
  - Метаданные и статистика отображаются во вкладке "Метаданные"
- Автоматическое добавление ответа модели в контекст (assistant message)
- **Автоматическое обновление вкладки "Контекст"** после получения ответа от модели
- Приложение должно корректно работать как с tools, так и без них

### 8.3. Автосохранение
- Автоматическое сохранение текущего состояния
- Периодическая синхронизация контекста
- Сохранение ответов после получения

---

## Этап 9: Дополнительные функции

### 9.1. Обработка ошибок
- Валидация входных данных
- Обработка ошибок API
- Пользовательские сообщения об ошибках

### 9.2. Улучшения UX
- Индикаторы загрузки
- Уведомления об успешных операциях
- Подтверждение удаления
- **Диалоговое окно для ошибок:** ошибки отображаются в модальном диалоговом окне с кнопкой закрыть и возможностью копировать текст ошибки
- История запросов (опционально)

### 9.3. Markdown рендеринг
- Библиотека для рендеринга markdown (например, marked.js)
- Подсветка синтаксиса для JSON
- Подсветка синтаксиса для кода в markdown

---

## Этап 10: Тестирование и документация

### 10.1. Тестирование
- Проверка всех endpoints
- Проверка сохранения/загрузки файлов
- Проверка работы с контекстами
- Проверка статистики

### 10.2. Документация
- README.md с инструкциями по установке и запуску
- Описание структуры конфигурационных файлов
- Примеры использования

---

## Порядок выполнения

1. **Этап 1** - Структура проекта и конфигурация
2. **Этап 2** - Backend: базовая инфраструктура
3. **Этап 3** - Backend: API endpoints
4. **Этап 4** - Backend: интеграция с LLM
5. **Этап 5** - Frontend: базовая структура
6. **Этап 6** - Frontend: верхняя панель
7. **Этап 7** - Frontend: нижняя панель
8. **Этап 8** - Frontend: интеграция с Backend
9. **Этап 9** - Дополнительные функции
10. **Этап 10** - Тестирование и документация

---

## Технические детали

### Формат контекста (JSON)
```json
{
  "name": "context_timestamp",
  "messages": [
    {
      "role": "user",
      "content": "..."
    },
    {
      "role": "assistant",
      "content": "...",
      "tool_calls": [
        {
          "id": "call_abc123",
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": "{\"location\": \"Boston\"}"
          }
        }
      ]
    },
    {
      "role": "tool",
      "tool_call_id": "call_abc123",
      "name": "get_weather",
      "content": "..."
    }
  ]
}
```

Примечание: формат messages полностью соответствует OpenAI API и поддерживает все опциональные поля:
- `content` может быть null для assistant messages только с tool_calls, но может быть не пустым одновременно с tool_calls
- `tool_calls` присутствует только для assistant messages и может быть одновременно с не пустым content
- `tool_call_id` и `name` присутствуют только для tool messages

### Формат статистики (JSON)
```json
{
  "timestamp": "...",
  "latency": 0.5,
  "time_to_first_token": 0.2,
  "total_time": 2.5,
  "response_tokens": 150,
  "response_words": 200,
  "response_characters": 1200,
  "avg_token_length": 8.0,
  "avg_word_tokens": 0.75,
  "context_tokens": 500,
  "inference_speed": 60.0
}
```

### Именование файлов
- Контексты: `{timestamp}.json` или пользовательское имя
- Промпты: пользовательское имя (расширение .txt или .md)
- Текущие данные: `current_content.md`, `current_tool_call.json`, `current_stats.json`

### Пример работы с tool calls

**Шаг 1: Запрос с определением tools**
```json
{
  "model": "gpt-4",
  "messages": [
    {"role": "user", "content": "Какая погода в Бостоне?"}
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Получить текущую погоду",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {"type": "string"}
          },
          "required": ["location"]
        }
      }
    }
  ]
}
```

**Шаг 2: Ответ модели с tool_calls**
```json
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": null,
      "tool_calls": [{
        "id": "call_abc123",
        "type": "function",
        "function": {
          "name": "get_weather",
          "arguments": "{\"location\": \"Boston\"}"
        }
      }]
    }
  }]
}
```

**Шаг 3: Добавление tool message в контекст (вручную пользователем)**
```json
{
  "role": "tool",
  "tool_call_id": "call_abc123",
  "name": "get_weather",
  "content": "{\"temperature\": 20, \"condition\": \"sunny\", \"humidity\": 60}"
}
```

**Шаг 4: Повторный запрос с полным контекстом**
```json
{
  "model": "gpt-4",
  "messages": [
    {"role": "user", "content": "Какая погода в Бостоне?"},
    {
      "role": "assistant",
      "content": null,
      "tool_calls": [{
        "id": "call_abc123",
        "type": "function",
        "function": {
          "name": "get_weather",
          "arguments": "{\"location\": \"Boston\"}"
        }
      }]
    },
    {
      "role": "tool",
      "tool_call_id": "call_abc123",
      "name": "get_weather",
      "content": "{\"temperature\": 20, \"condition\": \"sunny\", \"humidity\": 60}"
    }
  ]
}
```

**Шаг 5: Финальный ответ модели**
```json
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "В Бостоне сейчас солнечно, температура 20°C, влажность 60%."
    }
  }]
}
```

